"""
ğŸ¨ CulturalHub! ì „ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤
15ê°œ ë¬¸í™”ê¸°ê´€ API ëª¨ë‘ ì•ˆì „í•˜ê²Œ ì¤‘ì•™ ì§‘ì¤‘í™”í•˜ëŠ” ë¬¸í™” í—ˆë¸Œ ì‹œìŠ¤í…œì„ ê¸°ì¡´ ë„ë©”ì¸ì— í†µí•©
ğŸ”„ ì¦ë¶„ ìˆ˜ì§‘ìœ¼ë¡œ ì¤‘ë³µ ì—†ì´ ìƒˆë¡œìš´ ë°ì´í„°ë§Œ ì¶”ê°€
"""

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from sqlalchemy.orm import Session
from sqlalchemy import select, and_, or_, func
import sys
import os
from pathlib import Path

# CulturalHub ì‹œìŠ¤í…œ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent.parent.parent))
from cultural_hub_api_system import CulturalHubAPISystem

from app.domains.exhibition.models import Institution, Exhibition, DataSource
from app.core.config import settings

logger = logging.getLogger(__name__)


class CulturalHubExhibitionService:
    """ğŸ¨ CulturalHub ì „ì‹œ ë°ì´í„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤ - ì¤‘ë³µ ì—†ëŠ” ì¦ë¶„ ìˆ˜ì§‘"""
    
    def __init__(self, db: Session):
        self.db = db
        self.cultural_hub_system = CulturalHubAPISystem()
        
    async def collect_all_exhibitions_safely(
        self, 
        max_pages: int = 10, 
        use_sequential: bool = True,
        incremental: bool = True
    ) -> Dict[str, Any]:
        """ğŸ¨ 15ê°œ ë¬¸í™”ê¸°ê´€ API ëª¨ë‘ì—ì„œ ì•ˆì „í•˜ê²Œ ì „ì‹œ ë°ì´í„° ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)"""
        try:
            logger.info("ğŸ¨ CulturalHub ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # CulturalHub ì‹œìŠ¤í…œìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
            results = self.cultural_hub_system.run_cultural_hub_integration(
                max_pages=max_pages, 
                use_sequential=use_sequential
            )
            
            if results['total_data_count'] > 0:
                # ì¦ë¶„ ìˆ˜ì§‘ ëª¨ë“œë¡œ DBì— ì €ì¥
                save_results = await self._save_to_database_incremental(
                    results['integrated_data'], 
                    incremental=incremental
                )
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                await self._update_collection_stats(results)
                
                total_new = save_results['new_count']
                total_updated = save_results['updated_count'] 
                total_skipped = save_results['skipped_count']
                
                logger.info(f"ğŸ¨ CulturalHub ì¦ë¶„ ìˆ˜ì§‘ ì™„ë£Œ: ì‹ ê·œ {total_new}ê°œ, ì—…ë°ì´íŠ¸ {total_updated}ê°œ, ì¤‘ë³µ ìŠ¤í‚µ {total_skipped}ê°œ")
                
                return {
                    'success': True,
                    'total_collected': results['total_data_count'],
                    'total_new': total_new,
                    'total_updated': total_updated,
                    'total_skipped': total_skipped,
                    'working_apis': results['successful_apis'],
                    'total_apis': results['total_apis'],
                    'success_rate': (results['successful_apis']/results['total_apis'])*100,
                    'api_details': save_results['api_details'],
                    'details': results
                }
            else:
                logger.warning("ğŸ¨ CulturalHubì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return {
                    'success': False,
                    'total_collected': 0,
                    'total_new': 0,
                    'total_updated': 0,
                    'total_skipped': 0,
                    'working_apis': 0,
                    'total_apis': results.get('total_apis', 0),
                    'success_rate': 0,
                    'message': 'ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤',
                    'details': results
                }
                
        except Exception as e:
            logger.error(f"ğŸ¨ CulturalHub ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'success': False,
                'total_collected': 0,
                'total_new': 0,
                'total_updated': 0,
                'total_skipped': 0,
                'working_apis': 0,
                'total_apis': 0,
                'success_rate': 0,
                'message': f'ìˆ˜ì§‘ ì˜¤ë¥˜: {str(e)}'
            }
    
    async def _save_to_database_incremental(self, integrated_data: List[Dict], incremental: bool = True) -> Dict[str, Any]:
        """ğŸ”„ ì¦ë¶„ ëª¨ë“œë¡œ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ (ì¤‘ë³µ ì œê±°)"""
        save_stats = {
            'new_count': 0,
            'updated_count': 0,
            'skipped_count': 0,
            'api_details': {}
        }
        
        # API ì†ŒìŠ¤ë³„ë¡œ ë°ì´í„° ê·¸ë£¹í™”
        data_by_source = {}
        for item in integrated_data:
            api_source = item.get('api_source', 'unknown')
            if api_source not in data_by_source:
                data_by_source[api_source] = []
            data_by_source[api_source].append(item)
        
        for api_source, items in data_by_source.items():
            logger.info(f"ğŸ”„ {api_source} ë°ì´í„° ì²˜ë¦¬ ì¤‘: {len(items)}ê°œ")
            
            source_stats = {'new': 0, 'updated': 0, 'skipped': 0}
            
            for item_data in items:
                try:
                    # ê¸°ê´€ ì •ë³´ í™•ì¸/ìƒì„±
                    institution = await self._get_or_create_institution(
                        item_data.get('ì—°ê³„ê¸°ê´€ëª…', api_source)
                    )
                    
                    # ì¤‘ë³µ ê²€ì‚¬ ë° ì €ì¥
                    action = await self._process_exhibition_incremental(
                        institution, item_data, incremental
                    )
                    
                    if action == 'created':
                        source_stats['new'] += 1
                        save_stats['new_count'] += 1
                    elif action == 'updated':
                        source_stats['updated'] += 1
                        save_stats['updated_count'] += 1
                    elif action == 'skipped':
                        source_stats['skipped'] += 1
                        save_stats['skipped_count'] += 1
                        
                except Exception as e:
                    logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜ ({api_source}): {str(e)}")
                    source_stats['skipped'] += 1
                    save_stats['skipped_count'] += 1
            
            save_stats['api_details'][api_source] = source_stats
            logger.info(f"âœ… {api_source} ì™„ë£Œ: ì‹ ê·œ {source_stats['new']}ê°œ, ì—…ë°ì´íŠ¸ {source_stats['updated']}ê°œ, ìŠ¤í‚µ {source_stats['skipped']}ê°œ")
        
        # íŠ¸ëœì­ì…˜ ì»¤ë°‹
        try:
            self.db.commit()
            logger.info("ğŸ¨ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            self.db.rollback()
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            raise
        
        return save_stats
    
    async def _process_exhibition_incremental(
        self, 
        institution: Institution, 
        data: Dict, 
        incremental: bool = True
    ) -> str:
        """ğŸ”„ ì „ì‹œ ë°ì´í„° ì¦ë¶„ ì²˜ë¦¬ (ì¤‘ë³µ ê²€ì‚¬ ë° ì—…ë°ì´íŠ¸)"""
        
        # ë°ì´í„° ì •ê·œí™”
        normalized_data = self._normalize_cultural_data(data)
        
        # ì¤‘ë³µ ê²€ì‚¬ ê¸°ì¤€ ì„¤ì •
        title = normalized_data.get('title', '').strip()
        venue = normalized_data.get('venue', '').strip()
        api_source = normalized_data.get('api_source', '').strip()
        
        if not title:
            return 'skipped'  # ì œëª©ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        
        # ì¤‘ë³µ ê²€ì‚¬ ì¿¼ë¦¬ (ì—¬ëŸ¬ ì¡°ê±´ìœ¼ë¡œ ê²€ì‚¬)
        existing_query = self.db.query(Exhibition).filter(
            Exhibition.institution_id == institution.institution_id,
            Exhibition.title == title
        )
        
        # ì¶”ê°€ ì¡°ê±´ë“¤
        if venue:
            existing_query = existing_query.filter(Exhibition.venue == venue)
        if api_source:
            existing_query = existing_query.filter(Exhibition.api_source == api_source)
        
        existing_exhibition = existing_query.first()
        
        # ì™¸ë¶€ IDë¡œë„ ê²€ì‚¬
        external_id = normalized_data.get('external_id')
        if external_id and not existing_exhibition:
            existing_exhibition = self.db.query(Exhibition).filter(
                Exhibition.external_id == external_id,
                Exhibition.api_source == api_source
            ).first()
        
        current_time = datetime.now()
        
        if existing_exhibition:
            if incremental:
                # ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•„ìš”ì‹œ ì—…ë°ì´íŠ¸
                needs_update = self._needs_update(existing_exhibition, normalized_data)
                
                if needs_update:
                    # ì—…ë°ì´íŠ¸ ìˆ˜í–‰
                    for key, value in normalized_data.items():
                        if hasattr(existing_exhibition, key) and value is not None:
                            setattr(existing_exhibition, key, value)
                    
                    existing_exhibition.updated_at = current_time
                    existing_exhibition.collected_at = current_time
                    
                    return 'updated'
                else:
                    return 'skipped'  # ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”
            else:
                # ë¹„ì¦ë¶„ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°
                for key, value in normalized_data.items():
                    if hasattr(existing_exhibition, key) and value is not None:
                        setattr(existing_exhibition, key, value)
                
                existing_exhibition.updated_at = current_time
                existing_exhibition.collected_at = current_time
                
                return 'updated'
        else:
            # ìƒˆë¡œìš´ ì „ì‹œ ìƒì„±
            normalized_data['institution_id'] = institution.institution_id
            normalized_data['created_at'] = current_time
            normalized_data['collected_at'] = current_time
            
            exhibition = Exhibition(**normalized_data)
            self.db.add(exhibition)
            
            return 'created'
    
    def _needs_update(self, existing: Exhibition, new_data: Dict) -> bool:
        """ì—…ë°ì´íŠ¸ í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        # ì£¼ìš” í•„ë“œ ë³€ê²½ ê²€ì‚¬
        key_fields = ['description', 'start_date', 'end_date', 'price', 'website', 'image_url']
        
        for field in key_fields:
            new_value = new_data.get(field)
            existing_value = getattr(existing, field, None)
            
            # ìƒˆë¡œìš´ ê°’ì´ ìˆê³  ê¸°ì¡´ ê°’ê³¼ ë‹¤ë¥´ë©´ ì—…ë°ì´íŠ¸ í•„ìš”
            if new_value and new_value != existing_value:
                return True
        
        # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ì´ 24ì‹œê°„ ì´ìƒ ì§€ë‚¬ìœ¼ë©´ ì—…ë°ì´íŠ¸
        if existing.collected_at:
            time_diff = datetime.now() - existing.collected_at
            if time_diff.total_seconds() > 86400:  # 24ì‹œê°„
                return True
        
        return False
    
    def _normalize_cultural_data(self, data: Dict) -> Dict[str, Any]:
        """ğŸ¨ CulturalHub ë°ì´í„° ì •ê·œí™”"""
        normalized = {}
        
        # ê¸°ë³¸ í•„ë“œ ë§¤í•‘
        field_mapping = {
            'title': ['ì œëª©', 'title', 'subject'],
            'description': ['ì†Œê°œì„¤ëª…', 'description', 'content'],
            'venue': ['ì¥ì†Œ', 'venue', 'place'],
            'start_date': ['ì‹œì‘ì¼', 'start_date'],
            'end_date': ['ì¢…ë£Œì¼', 'end_date'],
            'period': ['ê¸°ê°„', 'period'],
            'time_info': ['ì‹œê°„', 'time_info', 'hour'],
            'price': ['ê´€ëŒë£Œí• ì¸ì •ë³´', 'price', 'pay'],
            'contact': ['ë¬¸ì˜', 'contact', 'tel'],
            'website': ['í™ˆí˜ì´ì§€ì£¼ì†Œ', 'website', 'homepage'],
            'image_url': ['ì´ë¯¸ì§€ì£¼ì†Œ', 'image_url', 'cover'],
            'category': ['ì¥ë¥´', 'category', 'event_gubun'],
            'organizer': ['ì£¼ìµœ', 'organizer', 'host'],
            'api_source': ['api_source'],
            'external_id': ['ì „ì‹œID', 'external_id', 'event_seq']
        }
        
        for target_field, source_fields in field_mapping.items():
            for source_field in source_fields:
                if source_field in data and data[source_field]:
                    value = data[source_field]
                    
                    # ë‚ ì§œ í•„ë“œ íŠ¹ë³„ ì²˜ë¦¬
                    if target_field in ['start_date', 'end_date'] and isinstance(value, str):
                        try:
                            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                            value = value.replace('.', '-').replace('/', '-')
                            if len(value) >= 10:
                                normalized[target_field] = datetime.strptime(value[:10], "%Y-%m-%d")
                        except ValueError:
                            normalized[target_field] = None
                    else:
                        # ë¬¸ìì—´ë¡œ ì €ì¥
                        normalized[target_field] = str(value).strip() if value else None
                    break
        
        # í•„ìˆ˜ í•„ë“œ ê¸°ë³¸ê°’ ì„¤ì •
        if not normalized.get('title'):
            normalized['title'] = normalized.get('venue', 'ì œëª© ì—†ìŒ')
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        normalized['source'] = 'cultural_hub'
        normalized['is_active'] = True
        
        return normalized
    
    async def _get_or_create_institution(self, name: str) -> Institution:
        """ê¸°ê´€ ì •ë³´ í™•ì¸/ìƒì„±"""
        if not name or name.strip() == '':
            name = 'ì •ë³´ ì—†ìŒ'
        
        name = name.strip()
        
        # ê¸°ì¡´ ê¸°ê´€ í™•ì¸
        existing = self.db.query(Institution).filter(Institution.name == name).first()
        
        if existing:
            return existing
        
        # ìƒˆ ê¸°ê´€ ìƒì„±
        institution = Institution(
            name=name,
            source='cultural_hub',
            created_at=datetime.now()
        )
        self.db.add(institution)
        self.db.flush()  # ID ìƒì„±ì„ ìœ„í•´ flush
        
        return institution
    
    async def test_all_cultural_apis(self, quick_test: bool = True) -> Dict[str, Any]:
        """ğŸ¨ CulturalHub 15ê°œ API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            logger.info("ğŸ¨ CulturalHub API í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ
            max_pages = 1 if quick_test else 3
            
            # CulturalHub ì‹œìŠ¤í…œìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            results = self.cultural_hub_system.run_cultural_hub_integration(
                max_pages=max_pages,
                use_sequential=True
            )
            
            return {
                'success': results['success'],
                'total_apis': results.get('total_apis', 0),
                'working_apis': results.get('successful_apis', 0),
                'success_rate': (results.get('successful_apis', 0) / results.get('total_apis', 1)) * 100,
                'test_data_count': results.get('total_data_count', 0),
                'details': results.get('collection_stats', {}),
                'message': f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {results.get('successful_apis', 0)}/{results.get('total_apis', 0)}ê°œ API ì„±ê³µ"
            }
            
        except Exception as e:
            logger.error(f"ğŸ¨ CulturalHub API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return {
                'success': False,
                'total_apis': 0,
                'working_apis': 0,
                'success_rate': 0,
                'test_data_count': 0,
                'message': f'í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}'
            }
    
    async def sync_specific_source(
        self, 
        source_name: Optional[str] = None, 
        force_update: bool = False
    ) -> Dict[str, Any]:
        """íŠ¹ì • ì†ŒìŠ¤ ë°ì´í„° ë™ê¸°í™”"""
        try:
            logger.info(f"ğŸ”„ {'ì „ì²´' if not source_name else source_name} ë™ê¸°í™” ì‹œì‘")
            
            # ì „ì²´ ìˆ˜ì§‘ í›„ íŠ¹ì • ì†ŒìŠ¤ë§Œ í•„í„°ë§
            results = self.cultural_hub_system.run_cultural_hub_integration(
                max_pages=5,
                use_sequential=True
            )
            
            if source_name:
                # íŠ¹ì • ì†ŒìŠ¤ë§Œ í•„í„°ë§
                filtered_data = [
                    item for item in results.get('integrated_data', [])
                    if item.get('api_source') == source_name
                ]
                
                if filtered_data:
                    sync_results = await self._save_to_database_incremental(
                        filtered_data, incremental=not force_update
                    )
                    
                    return {
                        'synced_count': len(filtered_data),
                        'updated_count': sync_results['updated_count'],
                        'new_count': sync_results['new_count'],
                        'details': sync_results
                    }
                else:
                    return {
                        'synced_count': 0,
                        'updated_count': 0,
                        'new_count': 0,
                        'message': f'{source_name} ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
                    }
            else:
                # ì „ì²´ ë™ê¸°í™”
                sync_results = await self._save_to_database_incremental(
                    results.get('integrated_data', []), 
                    incremental=not force_update
                )
                
                return {
                    'synced_count': results.get('total_data_count', 0),
                    'updated_count': sync_results['updated_count'],
                    'new_count': sync_results['new_count'],
                    'details': sync_results
                }
                
        except Exception as e:
            logger.error(f"ë™ê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return {
                'synced_count': 0,
                'updated_count': 0,
                'new_count': 0,
                'error': str(e)
            }
    
    async def _update_collection_stats(self, results: Dict) -> None:
        """ìˆ˜ì§‘ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            # ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ì—…ë°ì´íŠ¸
            for api_name in results.get('collection_stats', {}).keys():
                stmt = select(DataSource).where(DataSource.name.like(f"%{api_name}%"))
                result = self.db.execute(stmt)
                data_source = result.scalar_one_or_none()
                
                if data_source:
                    data_source.last_collected_at = datetime.now()
                    data_source.total_records = results['collection_stats'][api_name].get('count', 0)
                    data_source.success_rate = 100.0
                    data_source.status = 'active'
            
            self.db.commit()
            
        except Exception as e:
            logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
            self.db.rollback()


# ìŠ¤ì¼€ì¤„ëŸ¬ í•¨ìˆ˜ë“¤
async def schedule_cultural_hub_collection(db: Session):
    """ğŸ¨ ì •ê¸° CulturalHub ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    service = CulturalHubExhibitionService(db)
    return await service.collect_all_exhibitions_safely()


async def get_cultural_hub_status(db: Session):
    """ğŸ¨ CulturalHub ìˆ˜ì§‘ í˜„í™© ì¡°íšŒ"""
    service = CulturalHubExhibitionService(db)
    return await service.get_collection_status()


def setup_cultural_data_sources(db: Session):
    """ğŸ¨ ë¬¸í™” ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸° ì„¤ì •"""
    sources = [
        {
            "name": "ì˜ˆìˆ ì˜ì „ë‹¹",
            "source_type": "api",
            "description": "ì˜ˆìˆ ì˜ì „ë‹¹ ì „ì‹œì •ë³´ (ê³µì—°ì¥ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/API_CCA_149/request"
        },
        {
            "name": "ëŒ€í•œë¯¼êµ­ì—­ì‚¬ë°•ë¬¼ê´€",
            "source_type": "api",
            "description": "ëŒ€í•œë¯¼êµ­ì—­ì‚¬ë°•ë¬¼ê´€ íŠ¹ë³„ì „ì‹œ (êµ­ë¦½ë°•ë¬¼ê´€ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/meta2020/getMCHBspecial"
        },
        {
            "name": "êµ­ë¦½í•œê¸€ë°•ë¬¼ê´€",
            "source_type": "api",
            "description": "êµ­ë¦½í•œê¸€ë°•ë¬¼ê´€ ì „ì‹œì •ë³´ (êµ­ë¦½ë°•ë¬¼ê´€ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/meta2020/getNHMBex"
        },
        {
            "name": "í•œêµ­ë¬¸í™”ì˜ˆìˆ íšŒê´€ì—°í•©íšŒ",
            "source_type": "api",
            "description": "í•œêµ­ë¬¸í™”ì˜ˆìˆ íšŒê´€ì—°í•©íšŒ ê³µì—°ì „ì‹œì •ë³´ (ë¬¸í™”ì˜ˆìˆ íšŒê´€ / ì „êµ­)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/meta2020/getKOCAperf"
        },
        {
            "name": "í•œêµ­ê³µì˜ˆë””ìì¸ë¬¸í™”ì§„í¥ì›",
            "source_type": "api",
            "description": "í•œêµ­ê³µì˜ˆë””ìì¸ë¬¸í™”ì§„í¥ì› ì „ì‹œë„ë¡ (ì§„í¥ì› / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/meta8/getKCDA1503"
        },
        {
            "name": "í•œêµ­ë¬¸í™”ì˜ˆìˆ ìœ„ì›íšŒ",
            "source_type": "api",
            "description": "í•œêµ­ë¬¸í™”ì˜ˆìˆ ìœ„ì›íšŒ ì•„ë¥´ì½”ë¯¸ìˆ ê´€ì „ì‹œ (ë¯¸ìˆ ê´€ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/meta4/getARKA1202"
        },
        {
            "name": "ì „ì£¼ì‹œ",
            "source_type": "api",
            "description": "ì „ì£¼ì‹œ ê³µì—°ì „ì‹œì •ë³´ (ì§€ë°©ìì¹˜ë‹¨ì²´ / ì „ì£¼)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/other/getJEON5201"
        },
        {
            "name": "ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€",
            "source_type": "api",
            "description": "ì„œìš¸ì‹œë¦½ë¯¸ìˆ ê´€ ì „ì‹œì •ë³´ (ì‹œë¦½ë¯¸ìˆ ê´€ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/other/getSEMN5601"
        },
        {
            "name": "ë§ˆí¬ë¬¸í™”ì¬ë‹¨",
            "source_type": "api",
            "description": "ë§ˆí¬ë¬¸í™”ì¬ë‹¨ ë§ˆí¬ì•„íŠ¸ì„¼í„°ê³µì—°ì „ì‹œ (ë¬¸í™”ì¬ë‹¨ / ì„œìš¸ ë§ˆí¬)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/other/getMAPN0701"
        },
        {
            "name": "êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€",
            "source_type": "api",
            "description": "êµ­ë¦½í˜„ëŒ€ë¯¸ìˆ ê´€ ì „ì‹œì •ë³´ (êµ­ë¦½ë¯¸ìˆ ê´€ / ì„œìš¸/ê³¼ì²œ/ë•ìˆ˜ê¶/ì²­ì£¼)",
            "api_endpoint": "https://api.kcisa.kr/openapi/service/rest/moca/docMeta"
        },
        {
            "name": "í•œêµ­ë¬¸í™”ì •ë³´ì›",
            "source_type": "api",
            "description": "í•œêµ­ë¬¸í™”ì •ë³´ì› ì™¸ ì „ì‹œì •ë³´(í†µí•©) (í†µí•©ì •ë³´ì› / ì „êµ­)",
            "api_endpoint": "https://api.kcisa.kr/openapi/API_CCA_145/request"
        },
        {
            "name": "í•œêµ­ë¬¸í™”ì •ë³´ì›_ë°°ë¦¬ì–´í”„ë¦¬",
            "source_type": "api",
            "description": "í•œêµ­ë¬¸í™”ì •ë³´ì› ì „êµ­ ë¬¸í™”ì˜ˆìˆ ê´€ê´‘ì§€ ë°°ë¦¬ì–´í”„ë¦¬ ì •ë³´ (ì •ë³´ì› / ì „êµ­)",
            "api_endpoint": "https://api.kcisa.kr/openapi/API_TOU_049/request"
        },
        {
            "name": "êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€",
            "source_type": "api",
            "description": "êµ­ë¦½ì¤‘ì•™ë°•ë¬¼ê´€ ì™¸ ì „ì‹œë„ë¡ (êµ­ë¦½ë°•ë¬¼ê´€ / ì„œìš¸)",
            "api_endpoint": "https://api.kcisa.kr/API_CNV_049/request"
        },
        {
            "name": "ì œì£¼ë¬¸í™”ì˜ˆìˆ ì§„í¥ì›",
            "source_type": "api",
            "description": "ì œì£¼ë¬¸í™”ì˜ˆìˆ ì§„í¥ì› ê³µì—°/ì „ì‹œ ì •ë³´ (ì§€ì—­ì§„í¥ì› / ì œì£¼)",
            "api_endpoint": "http://www.jeju.go.kr/rest/JejuExhibitionService/getJejucultureExhibitionList"
        },
        {
            "name": "ëŒ€êµ¬ê´‘ì—­ì‹œ",
            "source_type": "api",
            "description": "ëŒ€êµ¬ê´‘ì—­ì‹œ ê³µì—°Â·ì „ì‹œ ì •ë³´ (ê´‘ì—­ì‹œ / ëŒ€êµ¬)",
            "api_endpoint": "https://dgfca.or.kr/api/daegu/cultural-events"
        }
    ]
    
    for source_data in sources:
        # ê¸°ì¡´ ì†ŒìŠ¤ í™•ì¸
        stmt = select(DataSource).where(DataSource.name == source_data["name"])
        result = db.execute(stmt)
        existing = result.scalar_one_or_none()
        
        if not existing:
            source = DataSource(**source_data)
            db.add(source)
    
    db.commit()
    logger.info("ğŸ¨ CulturalHub ë°ì´í„° ì†ŒìŠ¤ ì„¤ì • ì™„ë£Œ") 